{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "04d79886",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|████████████████████████████████████████████████████████| 33/33 [00:12<00:00,  2.70it/s]\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import argparse\n",
    "import torch\n",
    "from models.Preprocess_Llama import Model\n",
    "from data_provider.data_loader1 import Dataset_Preprocess\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "\n",
    "#parser = argparse.ArgumentParser(description='AutoTimes Preprocess')\n",
    "#parser.add_argument('--gpu', type=int, default=0, help='gpu id')\n",
    "#parser.add_argument('--llm_ckp_dir', type=str, default='./llama', help='llm checkpoints dir')\n",
    "#parser.add_argument('--dataset', type=str, default='ETTh1', \n",
    "#                    help='dataset to preprocess, options:[ETTh1, electricity, weather, traffic]')\n",
    "#args = parser.parse_args()\n",
    "\n",
    "# Define parameters directly\n",
    "gpu = 0  # GPU ID\n",
    "llm_ckp_dir = './llama'  # LLM checkpoint directory\n",
    "dataset_name = 'ETTh1'  # Dataset name, options: ['ETTh1', 'electricity', 'weather', 'traffic']\n",
    "\n",
    "\n",
    "#model = Model(args)\n",
    "model = Model(argparse.Namespace(\n",
    "    gpu=gpu,\n",
    "    llm_ckp_dir=llm_ckp_dir,\n",
    "    dataset=dataset_name\n",
    "))\n",
    "\n",
    "seq_len = 672\n",
    "label_len = 576\n",
    "pred_len = 96\n",
    "\n",
    "#assert args.dataset in ['ETTh1', 'electricity', 'weather', 'traffic']\n",
    "assert dataset_name in ['ETTh1', 'electricity', 'weather', 'traffic'], \"Invalid dataset name!\"\n",
    "\n",
    "# Load the dataset based on the selected dataset name\n",
    "if dataset_name == 'ETTh1':\n",
    "    data_set = Dataset_Preprocess(\n",
    "        root_path='./dataset/ETT-small/',\n",
    "        data_path='ETTh1.csv',\n",
    "        size=[seq_len, label_len, pred_len])\n",
    "elif dataset_name == 'electricity':\n",
    "    data_set = Dataset_Preprocess(\n",
    "        root_path='./dataset/electricity/',\n",
    "        data_path='electricity.csv',\n",
    "        size=[seq_len, label_len, pred_len])\n",
    "elif dataset_name == 'weather':\n",
    "    data_set = Dataset_Preprocess(\n",
    "        root_path='./dataset/weather/',\n",
    "        data_path='weather.csv',\n",
    "        size=[seq_len, label_len, pred_len])\n",
    "elif dataset_name == 'traffic':\n",
    "    data_set = Dataset_Preprocess(\n",
    "        root_path='./dataset/traffic/',\n",
    "        data_path='traffic.csv',\n",
    "        size=[seq_len, label_len, pred_len])\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "916adfd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_set = Dataset_Preprocess(\n",
    "    root_path='./dataset/ETT-small/',\n",
    "    data_path='ETTh1.csv',\n",
    "    size=[seq_len, label_len, pred_len])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a6b1f96b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataLoader for the dataset\n",
    "data_loader = DataLoader(\n",
    "    data_set,\n",
    "    batch_size=128,\n",
    "    shuffle=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "661ca3d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17420\n",
      "17420\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['This is Time Series from 2016-07-01 00:00:00 to 2016-07-04 23:00:00', 'This is Time Series from 2016-07-01 01:00:00 to 2016-07-05 00:00:00', 'This is Time Series from 2016-07-01 02:00:00 to 2016-07-05 01:00:00', 'This is Time Series from 2016-07-01 03:00:00 to 2016-07-05 02:00:00', 'This is Time Series from 2016-07-01 04:00:00 to 2016-07-05 03:00:00', 'This is Time Series from 2016-07-01 05:00:00 to 2016-07-05 04:00:00', 'This is Time Series from 2016-07-01 06:00:00 to 2016-07-05 05:00:00', 'This is Time Series from 2016-07-01 07:00:00 to 2016-07-05 06:00:00', 'This is Time Series from 2016-07-01 08:00:00 to 2016-07-05 07:00:00', 'This is Time Series from 2016-07-01 09:00:00 to 2016-07-05 08:00:00', 'This is Time Series from 2016-07-01 10:00:00 to 2016-07-05 09:00:00', 'This is Time Series from 2016-07-01 11:00:00 to 2016-07-05 10:00:00', 'This is Time Series from 2016-07-01 12:00:00 to 2016-07-05 11:00:00', 'This is Time Series from 2016-07-01 13:00:00 to 2016-07-05 12:00:00', 'This is Time Series from 2016-07-01 14:00:00 to 2016-07-05 13:00:00', 'This is Time Series from 2016-07-01 15:00:00 to 2016-07-05 14:00:00', 'This is Time Series from 2016-07-01 16:00:00 to 2016-07-05 15:00:00', 'This is Time Series from 2016-07-01 17:00:00 to 2016-07-05 16:00:00', 'This is Time Series from 2016-07-01 18:00:00 to 2016-07-05 17:00:00', 'This is Time Series from 2016-07-01 19:00:00 to 2016-07-05 18:00:00', 'This is Time Series from 2016-07-01 20:00:00 to 2016-07-05 19:00:00', 'This is Time Series from 2016-07-01 21:00:00 to 2016-07-05 20:00:00', 'This is Time Series from 2016-07-01 22:00:00 to 2016-07-05 21:00:00', 'This is Time Series from 2016-07-01 23:00:00 to 2016-07-05 22:00:00', 'This is Time Series from 2016-07-02 00:00:00 to 2016-07-05 23:00:00', 'This is Time Series from 2016-07-02 01:00:00 to 2016-07-06 00:00:00', 'This is Time Series from 2016-07-02 02:00:00 to 2016-07-06 01:00:00', 'This is Time Series from 2016-07-02 03:00:00 to 2016-07-06 02:00:00', 'This is Time Series from 2016-07-02 04:00:00 to 2016-07-06 03:00:00', 'This is Time Series from 2016-07-02 05:00:00 to 2016-07-06 04:00:00', 'This is Time Series from 2016-07-02 06:00:00 to 2016-07-06 05:00:00', 'This is Time Series from 2016-07-02 07:00:00 to 2016-07-06 06:00:00', 'This is Time Series from 2016-07-02 08:00:00 to 2016-07-06 07:00:00', 'This is Time Series from 2016-07-02 09:00:00 to 2016-07-06 08:00:00', 'This is Time Series from 2016-07-02 10:00:00 to 2016-07-06 09:00:00', 'This is Time Series from 2016-07-02 11:00:00 to 2016-07-06 10:00:00', 'This is Time Series from 2016-07-02 12:00:00 to 2016-07-06 11:00:00', 'This is Time Series from 2016-07-02 13:00:00 to 2016-07-06 12:00:00', 'This is Time Series from 2016-07-02 14:00:00 to 2016-07-06 13:00:00', 'This is Time Series from 2016-07-02 15:00:00 to 2016-07-06 14:00:00', 'This is Time Series from 2016-07-02 16:00:00 to 2016-07-06 15:00:00', 'This is Time Series from 2016-07-02 17:00:00 to 2016-07-06 16:00:00', 'This is Time Series from 2016-07-02 18:00:00 to 2016-07-06 17:00:00', 'This is Time Series from 2016-07-02 19:00:00 to 2016-07-06 18:00:00', 'This is Time Series from 2016-07-02 20:00:00 to 2016-07-06 19:00:00', 'This is Time Series from 2016-07-02 21:00:00 to 2016-07-06 20:00:00', 'This is Time Series from 2016-07-02 22:00:00 to 2016-07-06 21:00:00', 'This is Time Series from 2016-07-02 23:00:00 to 2016-07-06 22:00:00', 'This is Time Series from 2016-07-03 00:00:00 to 2016-07-06 23:00:00', 'This is Time Series from 2016-07-03 01:00:00 to 2016-07-07 00:00:00', 'This is Time Series from 2016-07-03 02:00:00 to 2016-07-07 01:00:00', 'This is Time Series from 2016-07-03 03:00:00 to 2016-07-07 02:00:00', 'This is Time Series from 2016-07-03 04:00:00 to 2016-07-07 03:00:00', 'This is Time Series from 2016-07-03 05:00:00 to 2016-07-07 04:00:00', 'This is Time Series from 2016-07-03 06:00:00 to 2016-07-07 05:00:00', 'This is Time Series from 2016-07-03 07:00:00 to 2016-07-07 06:00:00', 'This is Time Series from 2016-07-03 08:00:00 to 2016-07-07 07:00:00', 'This is Time Series from 2016-07-03 09:00:00 to 2016-07-07 08:00:00', 'This is Time Series from 2016-07-03 10:00:00 to 2016-07-07 09:00:00', 'This is Time Series from 2016-07-03 11:00:00 to 2016-07-07 10:00:00', 'This is Time Series from 2016-07-03 12:00:00 to 2016-07-07 11:00:00', 'This is Time Series from 2016-07-03 13:00:00 to 2016-07-07 12:00:00', 'This is Time Series from 2016-07-03 14:00:00 to 2016-07-07 13:00:00', 'This is Time Series from 2016-07-03 15:00:00 to 2016-07-07 14:00:00', 'This is Time Series from 2016-07-03 16:00:00 to 2016-07-07 15:00:00', 'This is Time Series from 2016-07-03 17:00:00 to 2016-07-07 16:00:00', 'This is Time Series from 2016-07-03 18:00:00 to 2016-07-07 17:00:00', 'This is Time Series from 2016-07-03 19:00:00 to 2016-07-07 18:00:00', 'This is Time Series from 2016-07-03 20:00:00 to 2016-07-07 19:00:00', 'This is Time Series from 2016-07-03 21:00:00 to 2016-07-07 20:00:00', 'This is Time Series from 2016-07-03 22:00:00 to 2016-07-07 21:00:00', 'This is Time Series from 2016-07-03 23:00:00 to 2016-07-07 22:00:00', 'This is Time Series from 2016-07-04 00:00:00 to 2016-07-07 23:00:00', 'This is Time Series from 2016-07-04 01:00:00 to 2016-07-08 00:00:00', 'This is Time Series from 2016-07-04 02:00:00 to 2016-07-08 01:00:00', 'This is Time Series from 2016-07-04 03:00:00 to 2016-07-08 02:00:00', 'This is Time Series from 2016-07-04 04:00:00 to 2016-07-08 03:00:00', 'This is Time Series from 2016-07-04 05:00:00 to 2016-07-08 04:00:00', 'This is Time Series from 2016-07-04 06:00:00 to 2016-07-08 05:00:00', 'This is Time Series from 2016-07-04 07:00:00 to 2016-07-08 06:00:00', 'This is Time Series from 2016-07-04 08:00:00 to 2016-07-08 07:00:00', 'This is Time Series from 2016-07-04 09:00:00 to 2016-07-08 08:00:00', 'This is Time Series from 2016-07-04 10:00:00 to 2016-07-08 09:00:00', 'This is Time Series from 2016-07-04 11:00:00 to 2016-07-08 10:00:00', 'This is Time Series from 2016-07-04 12:00:00 to 2016-07-08 11:00:00', 'This is Time Series from 2016-07-04 13:00:00 to 2016-07-08 12:00:00', 'This is Time Series from 2016-07-04 14:00:00 to 2016-07-08 13:00:00', 'This is Time Series from 2016-07-04 15:00:00 to 2016-07-08 14:00:00', 'This is Time Series from 2016-07-04 16:00:00 to 2016-07-08 15:00:00', 'This is Time Series from 2016-07-04 17:00:00 to 2016-07-08 16:00:00', 'This is Time Series from 2016-07-04 18:00:00 to 2016-07-08 17:00:00', 'This is Time Series from 2016-07-04 19:00:00 to 2016-07-08 18:00:00', 'This is Time Series from 2016-07-04 20:00:00 to 2016-07-08 19:00:00', 'This is Time Series from 2016-07-04 21:00:00 to 2016-07-08 20:00:00', 'This is Time Series from 2016-07-04 22:00:00 to 2016-07-08 21:00:00', 'This is Time Series from 2016-07-04 23:00:00 to 2016-07-08 22:00:00', 'This is Time Series from 2016-07-05 00:00:00 to 2016-07-08 23:00:00', 'This is Time Series from 2016-07-05 01:00:00 to 2016-07-09 00:00:00', 'This is Time Series from 2016-07-05 02:00:00 to 2016-07-09 01:00:00', 'This is Time Series from 2016-07-05 03:00:00 to 2016-07-09 02:00:00', 'This is Time Series from 2016-07-05 04:00:00 to 2016-07-09 03:00:00', 'This is Time Series from 2016-07-05 05:00:00 to 2016-07-09 04:00:00', 'This is Time Series from 2016-07-05 06:00:00 to 2016-07-09 05:00:00', 'This is Time Series from 2016-07-05 07:00:00 to 2016-07-09 06:00:00', 'This is Time Series from 2016-07-05 08:00:00 to 2016-07-09 07:00:00', 'This is Time Series from 2016-07-05 09:00:00 to 2016-07-09 08:00:00', 'This is Time Series from 2016-07-05 10:00:00 to 2016-07-09 09:00:00', 'This is Time Series from 2016-07-05 11:00:00 to 2016-07-09 10:00:00', 'This is Time Series from 2016-07-05 12:00:00 to 2016-07-09 11:00:00', 'This is Time Series from 2016-07-05 13:00:00 to 2016-07-09 12:00:00', 'This is Time Series from 2016-07-05 14:00:00 to 2016-07-09 13:00:00', 'This is Time Series from 2016-07-05 15:00:00 to 2016-07-09 14:00:00', 'This is Time Series from 2016-07-05 16:00:00 to 2016-07-09 15:00:00', 'This is Time Series from 2016-07-05 17:00:00 to 2016-07-09 16:00:00', 'This is Time Series from 2016-07-05 18:00:00 to 2016-07-09 17:00:00', 'This is Time Series from 2016-07-05 19:00:00 to 2016-07-09 18:00:00', 'This is Time Series from 2016-07-05 20:00:00 to 2016-07-09 19:00:00', 'This is Time Series from 2016-07-05 21:00:00 to 2016-07-09 20:00:00', 'This is Time Series from 2016-07-05 22:00:00 to 2016-07-09 21:00:00', 'This is Time Series from 2016-07-05 23:00:00 to 2016-07-09 22:00:00', 'This is Time Series from 2016-07-06 00:00:00 to 2016-07-09 23:00:00', 'This is Time Series from 2016-07-06 01:00:00 to 2016-07-10 00:00:00', 'This is Time Series from 2016-07-06 02:00:00 to 2016-07-10 01:00:00', 'This is Time Series from 2016-07-06 03:00:00 to 2016-07-10 02:00:00', 'This is Time Series from 2016-07-06 04:00:00 to 2016-07-10 03:00:00', 'This is Time Series from 2016-07-06 05:00:00 to 2016-07-10 04:00:00', 'This is Time Series from 2016-07-06 06:00:00 to 2016-07-10 05:00:00', 'This is Time Series from 2016-07-06 07:00:00 to 2016-07-10 06:00:00'] 128\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "1it [00:01,  1.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['This is Time Series from 2016-07-06 08:00:00 to 2016-07-10 07:00:00', 'This is Time Series from 2016-07-06 09:00:00 to 2016-07-10 08:00:00', 'This is Time Series from 2016-07-06 10:00:00 to 2016-07-10 09:00:00', 'This is Time Series from 2016-07-06 11:00:00 to 2016-07-10 10:00:00', 'This is Time Series from 2016-07-06 12:00:00 to 2016-07-10 11:00:00', 'This is Time Series from 2016-07-06 13:00:00 to 2016-07-10 12:00:00', 'This is Time Series from 2016-07-06 14:00:00 to 2016-07-10 13:00:00', 'This is Time Series from 2016-07-06 15:00:00 to 2016-07-10 14:00:00', 'This is Time Series from 2016-07-06 16:00:00 to 2016-07-10 15:00:00', 'This is Time Series from 2016-07-06 17:00:00 to 2016-07-10 16:00:00', 'This is Time Series from 2016-07-06 18:00:00 to 2016-07-10 17:00:00', 'This is Time Series from 2016-07-06 19:00:00 to 2016-07-10 18:00:00', 'This is Time Series from 2016-07-06 20:00:00 to 2016-07-10 19:00:00', 'This is Time Series from 2016-07-06 21:00:00 to 2016-07-10 20:00:00', 'This is Time Series from 2016-07-06 22:00:00 to 2016-07-10 21:00:00', 'This is Time Series from 2016-07-06 23:00:00 to 2016-07-10 22:00:00', 'This is Time Series from 2016-07-07 00:00:00 to 2016-07-10 23:00:00', 'This is Time Series from 2016-07-07 01:00:00 to 2016-07-11 00:00:00', 'This is Time Series from 2016-07-07 02:00:00 to 2016-07-11 01:00:00', 'This is Time Series from 2016-07-07 03:00:00 to 2016-07-11 02:00:00', 'This is Time Series from 2016-07-07 04:00:00 to 2016-07-11 03:00:00', 'This is Time Series from 2016-07-07 05:00:00 to 2016-07-11 04:00:00', 'This is Time Series from 2016-07-07 06:00:00 to 2016-07-11 05:00:00', 'This is Time Series from 2016-07-07 07:00:00 to 2016-07-11 06:00:00', 'This is Time Series from 2016-07-07 08:00:00 to 2016-07-11 07:00:00', 'This is Time Series from 2016-07-07 09:00:00 to 2016-07-11 08:00:00', 'This is Time Series from 2016-07-07 10:00:00 to 2016-07-11 09:00:00', 'This is Time Series from 2016-07-07 11:00:00 to 2016-07-11 10:00:00', 'This is Time Series from 2016-07-07 12:00:00 to 2016-07-11 11:00:00', 'This is Time Series from 2016-07-07 13:00:00 to 2016-07-11 12:00:00', 'This is Time Series from 2016-07-07 14:00:00 to 2016-07-11 13:00:00', 'This is Time Series from 2016-07-07 15:00:00 to 2016-07-11 14:00:00', 'This is Time Series from 2016-07-07 16:00:00 to 2016-07-11 15:00:00', 'This is Time Series from 2016-07-07 17:00:00 to 2016-07-11 16:00:00', 'This is Time Series from 2016-07-07 18:00:00 to 2016-07-11 17:00:00', 'This is Time Series from 2016-07-07 19:00:00 to 2016-07-11 18:00:00', 'This is Time Series from 2016-07-07 20:00:00 to 2016-07-11 19:00:00', 'This is Time Series from 2016-07-07 21:00:00 to 2016-07-11 20:00:00', 'This is Time Series from 2016-07-07 22:00:00 to 2016-07-11 21:00:00', 'This is Time Series from 2016-07-07 23:00:00 to 2016-07-11 22:00:00', 'This is Time Series from 2016-07-08 00:00:00 to 2016-07-11 23:00:00', 'This is Time Series from 2016-07-08 01:00:00 to 2016-07-12 00:00:00', 'This is Time Series from 2016-07-08 02:00:00 to 2016-07-12 01:00:00', 'This is Time Series from 2016-07-08 03:00:00 to 2016-07-12 02:00:00', 'This is Time Series from 2016-07-08 04:00:00 to 2016-07-12 03:00:00', 'This is Time Series from 2016-07-08 05:00:00 to 2016-07-12 04:00:00', 'This is Time Series from 2016-07-08 06:00:00 to 2016-07-12 05:00:00', 'This is Time Series from 2016-07-08 07:00:00 to 2016-07-12 06:00:00', 'This is Time Series from 2016-07-08 08:00:00 to 2016-07-12 07:00:00', 'This is Time Series from 2016-07-08 09:00:00 to 2016-07-12 08:00:00', 'This is Time Series from 2016-07-08 10:00:00 to 2016-07-12 09:00:00', 'This is Time Series from 2016-07-08 11:00:00 to 2016-07-12 10:00:00', 'This is Time Series from 2016-07-08 12:00:00 to 2016-07-12 11:00:00', 'This is Time Series from 2016-07-08 13:00:00 to 2016-07-12 12:00:00', 'This is Time Series from 2016-07-08 14:00:00 to 2016-07-12 13:00:00', 'This is Time Series from 2016-07-08 15:00:00 to 2016-07-12 14:00:00', 'This is Time Series from 2016-07-08 16:00:00 to 2016-07-12 15:00:00', 'This is Time Series from 2016-07-08 17:00:00 to 2016-07-12 16:00:00', 'This is Time Series from 2016-07-08 18:00:00 to 2016-07-12 17:00:00', 'This is Time Series from 2016-07-08 19:00:00 to 2016-07-12 18:00:00', 'This is Time Series from 2016-07-08 20:00:00 to 2016-07-12 19:00:00', 'This is Time Series from 2016-07-08 21:00:00 to 2016-07-12 20:00:00', 'This is Time Series from 2016-07-08 22:00:00 to 2016-07-12 21:00:00', 'This is Time Series from 2016-07-08 23:00:00 to 2016-07-12 22:00:00', 'This is Time Series from 2016-07-09 00:00:00 to 2016-07-12 23:00:00', 'This is Time Series from 2016-07-09 01:00:00 to 2016-07-13 00:00:00', 'This is Time Series from 2016-07-09 02:00:00 to 2016-07-13 01:00:00', 'This is Time Series from 2016-07-09 03:00:00 to 2016-07-13 02:00:00', 'This is Time Series from 2016-07-09 04:00:00 to 2016-07-13 03:00:00', 'This is Time Series from 2016-07-09 05:00:00 to 2016-07-13 04:00:00', 'This is Time Series from 2016-07-09 06:00:00 to 2016-07-13 05:00:00', 'This is Time Series from 2016-07-09 07:00:00 to 2016-07-13 06:00:00', 'This is Time Series from 2016-07-09 08:00:00 to 2016-07-13 07:00:00', 'This is Time Series from 2016-07-09 09:00:00 to 2016-07-13 08:00:00', 'This is Time Series from 2016-07-09 10:00:00 to 2016-07-13 09:00:00', 'This is Time Series from 2016-07-09 11:00:00 to 2016-07-13 10:00:00', 'This is Time Series from 2016-07-09 12:00:00 to 2016-07-13 11:00:00', 'This is Time Series from 2016-07-09 13:00:00 to 2016-07-13 12:00:00', 'This is Time Series from 2016-07-09 14:00:00 to 2016-07-13 13:00:00', 'This is Time Series from 2016-07-09 15:00:00 to 2016-07-13 14:00:00', 'This is Time Series from 2016-07-09 16:00:00 to 2016-07-13 15:00:00', 'This is Time Series from 2016-07-09 17:00:00 to 2016-07-13 16:00:00', 'This is Time Series from 2016-07-09 18:00:00 to 2016-07-13 17:00:00', 'This is Time Series from 2016-07-09 19:00:00 to 2016-07-13 18:00:00', 'This is Time Series from 2016-07-09 20:00:00 to 2016-07-13 19:00:00', 'This is Time Series from 2016-07-09 21:00:00 to 2016-07-13 20:00:00', 'This is Time Series from 2016-07-09 22:00:00 to 2016-07-13 21:00:00', 'This is Time Series from 2016-07-09 23:00:00 to 2016-07-13 22:00:00', 'This is Time Series from 2016-07-10 00:00:00 to 2016-07-13 23:00:00', 'This is Time Series from 2016-07-10 01:00:00 to 2016-07-14 00:00:00', 'This is Time Series from 2016-07-10 02:00:00 to 2016-07-14 01:00:00', 'This is Time Series from 2016-07-10 03:00:00 to 2016-07-14 02:00:00', 'This is Time Series from 2016-07-10 04:00:00 to 2016-07-14 03:00:00', 'This is Time Series from 2016-07-10 05:00:00 to 2016-07-14 04:00:00', 'This is Time Series from 2016-07-10 06:00:00 to 2016-07-14 05:00:00', 'This is Time Series from 2016-07-10 07:00:00 to 2016-07-14 06:00:00', 'This is Time Series from 2016-07-10 08:00:00 to 2016-07-14 07:00:00', 'This is Time Series from 2016-07-10 09:00:00 to 2016-07-14 08:00:00', 'This is Time Series from 2016-07-10 10:00:00 to 2016-07-14 09:00:00', 'This is Time Series from 2016-07-10 11:00:00 to 2016-07-14 10:00:00', 'This is Time Series from 2016-07-10 12:00:00 to 2016-07-14 11:00:00', 'This is Time Series from 2016-07-10 13:00:00 to 2016-07-14 12:00:00', 'This is Time Series from 2016-07-10 14:00:00 to 2016-07-14 13:00:00', 'This is Time Series from 2016-07-10 15:00:00 to 2016-07-14 14:00:00', 'This is Time Series from 2016-07-10 16:00:00 to 2016-07-14 15:00:00', 'This is Time Series from 2016-07-10 17:00:00 to 2016-07-14 16:00:00', 'This is Time Series from 2016-07-10 18:00:00 to 2016-07-14 17:00:00', 'This is Time Series from 2016-07-10 19:00:00 to 2016-07-14 18:00:00', 'This is Time Series from 2016-07-10 20:00:00 to 2016-07-14 19:00:00', 'This is Time Series from 2016-07-10 21:00:00 to 2016-07-14 20:00:00', 'This is Time Series from 2016-07-10 22:00:00 to 2016-07-14 21:00:00', 'This is Time Series from 2016-07-10 23:00:00 to 2016-07-14 22:00:00', 'This is Time Series from 2016-07-11 00:00:00 to 2016-07-14 23:00:00', 'This is Time Series from 2016-07-11 01:00:00 to 2016-07-15 00:00:00', 'This is Time Series from 2016-07-11 02:00:00 to 2016-07-15 01:00:00', 'This is Time Series from 2016-07-11 03:00:00 to 2016-07-15 02:00:00', 'This is Time Series from 2016-07-11 04:00:00 to 2016-07-15 03:00:00', 'This is Time Series from 2016-07-11 05:00:00 to 2016-07-15 04:00:00', 'This is Time Series from 2016-07-11 06:00:00 to 2016-07-15 05:00:00', 'This is Time Series from 2016-07-11 07:00:00 to 2016-07-15 06:00:00', 'This is Time Series from 2016-07-11 08:00:00 to 2016-07-15 07:00:00', 'This is Time Series from 2016-07-11 09:00:00 to 2016-07-15 08:00:00', 'This is Time Series from 2016-07-11 10:00:00 to 2016-07-15 09:00:00', 'This is Time Series from 2016-07-11 11:00:00 to 2016-07-15 10:00:00', 'This is Time Series from 2016-07-11 12:00:00 to 2016-07-15 11:00:00', 'This is Time Series from 2016-07-11 13:00:00 to 2016-07-15 12:00:00', 'This is Time Series from 2016-07-11 14:00:00 to 2016-07-15 13:00:00', 'This is Time Series from 2016-07-11 15:00:00 to 2016-07-15 14:00:00'] 128\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:03,  3.12s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 8\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;28mprint\u001b[39m(data,\u001b[38;5;28mlen\u001b[39m(data))\n\u001b[1;32m      7\u001b[0m     output \u001b[38;5;241m=\u001b[39m model(data)\n\u001b[0;32m----> 8\u001b[0m     output_list\u001b[38;5;241m.\u001b[39mappend(\u001b[43moutput\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdetach\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcpu\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;66;03m#print(len(output_list[0]))\u001b[39;00m\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;66;03m#if idx>1:break\u001b[39;00m\n\u001b[1;32m     11\u001b[0m result \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat(output_list, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "print(len(data_set.data_stamp))\n",
    "print(data_set.tot_len)\n",
    "save_dir_path = './dataset/'\n",
    "output_list = []\n",
    "for idx, data in tqdm(enumerate(data_loader)):\n",
    "    print(data,len(data))\n",
    "    output = model(data)\n",
    "    output_list.append(output.detach().cpu())\n",
    "    #print(len(output_list[0]))\n",
    "    #if idx>1:break\n",
    "result = torch.cat(output_list, dim=0)\n",
    "print(result.shape)\n",
    "torch.save(result, save_dir_path + f'/{args.dataset}.ptt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aa07d758",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/dssg/home/lcx/python_install/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/dssg/home/lcx/python_install/lib/python3.8/site-packages/torchvision/datapoints/__init__.py:12: UserWarning: The torchvision.datapoints and torchvision.transforms.v2 namespaces are still Beta. While we do not expect major breaking changes, some APIs may still change according to user feedback. Please submit any feedback you may have in this issue: https://github.com/pytorch/vision/issues/6753, and you can also check out https://github.com/pytorch/vision/issues/7319 to learn more about the APIs that we suspect might involve future changes. You can silence this warning by calling torchvision.disable_beta_transforms_warning().\n",
      "  warnings.warn(_BETA_TRANSFORMS_WARNING)\n",
      "/dssg/home/lcx/python_install/lib/python3.8/site-packages/torchvision/transforms/v2/__init__.py:54: UserWarning: The torchvision.datapoints and torchvision.transforms.v2 namespaces are still Beta. While we do not expect major breaking changes, some APIs may still change according to user feedback. Please submit any feedback you may have in this issue: https://github.com/pytorch/vision/issues/6753, and you can also check out https://github.com/pytorch/vision/issues/7319 to learn more about the APIs that we suspect might involve future changes. You can silence this warning by calling torchvision.disable_beta_transforms_warning().\n",
      "  warnings.warn(_BETA_TRANSFORMS_WARNING)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:04<00:00,  2.34s/it]\n"
     ]
    }
   ],
   "source": [
    "#os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n",
    "# Import necessary libraries\n",
    "import argparse\n",
    "import torch\n",
    "from models.Preprocess_DeepSeek import Model\n",
    "from data_provider.data_loader1 import Dataset_Preprocess_Sat\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "\n",
    "\n",
    "gpu = 2  # GPU ID\n",
    "llm_ckp_dir = './deepseek'  # LLM checkpoint directory\n",
    "data_dir = './dataset/sats_ds_2hr'  # Dataset name, options: ['ETTh1', 'electricity', 'weather', 'traffic']\n",
    "\n",
    "\n",
    "#model = Model(args)\n",
    "model = Model(argparse.Namespace(\n",
    "    gpu=gpu,\n",
    "    llm_ckp_dir=llm_ckp_dir,\n",
    "    data_dir=data_dir\n",
    "))\n",
    "\n",
    "\n",
    "# Sequence length parameters\n",
    "seq_len = 12\n",
    "label_len = 10\n",
    "pred_len = 2\n",
    "\n",
    "# Placeholder for all processed results\n",
    "save_dir_path = './saved_sat_stamps_ds_2hr_coord'\n",
    "os.makedirs(save_dir_path, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4eaba2de",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 43014_coordinates.csv...\n",
      "43014_coordinates.csv.pt exists.\n",
      "Processing 47715_coordinates.csv...\n",
      "47715_coordinates.csv.pt exists.\n",
      "Processing 27151_coordinates.csv...\n",
      "27151_coordinates.csv.pt exists.\n",
      "Processing 47702_coordinates.csv...\n",
      "47702_coordinates.csv.pt exists.\n",
      "Processing 43723_coordinates.csv...\n",
      "43723_coordinates.csv.pt exists.\n",
      "Processing 38710_coordinates.csv...\n",
      "38710_coordinates.csv.pt exists.\n",
      "Processing 43722_coordinates.csv...\n",
      "43722_coordinates.csv.pt exists.\n",
      "Processing 48414_coordinates.csv...\n",
      "48414_coordinates.csv.pt exists.\n",
      "Processing 46825_coordinates.csv...\n",
      "46825_coordinates.csv.pt exists.\n",
      "Processing 40539_coordinates.csv...\n",
      "40539_coordinates.csv.pt exists.\n",
      "Processing 11251_coordinates.csv...\n",
      "11251_coordinates.csv.pt exists.\n",
      "Processing 46929_coordinates.csv...\n",
      "46929_coordinates.csv.pt exists.\n",
      "Processing 47853_coordinates.csv...\n",
      "47853_coordinates.csv.pt exists.\n",
      "Processing 43617_coordinates.csv...\n",
      "43617_coordinates.csv.pt exists.\n",
      "Processing 42781_coordinates.csv...\n",
      "42781_coordinates.csv.pt exists.\n",
      "Processing 47316_coordinates.csv...\n",
      "47316_coordinates.csv.pt exists.\n",
      "Processing 39186_coordinates.csv...\n",
      "39186_coordinates.csv.pt exists.\n",
      "Processing 45858_coordinates.csv...\n",
      "45858_coordinates.csv.pt exists.\n",
      "Processing 47512_coordinates.csv...\n",
      "47512_coordinates.csv.pt exists.\n",
      "Processing 43734_coordinates.csv...\n",
      "43734_coordinates.csv.pt exists.\n",
      "Processing 44413_coordinates.csv...\n",
      "44413_coordinates.csv.pt exists.\n",
      "Processing 47492_coordinates.csv...\n",
      "47492_coordinates.csv.pt exists.\n",
      "Processing 46838_coordinates.csv...\n",
      "46838_coordinates.csv.pt exists.\n",
      "Processing 39363_coordinates.csv...\n",
      "39363_coordinates.csv.pt exists.\n",
      "Processing 46840_coordinates.csv...\n",
      "46840_coordinates.csv.pt exists.\n",
      "Processing 43133_coordinates.csv...\n",
      "43133_coordinates.csv.pt exists.\n",
      "Processing 44409_coordinates.csv...\n",
      "44409_coordinates.csv.pt exists.\n",
      "Processing 42789_coordinates.csv...\n",
      "42789_coordinates.csv.pt exists.\n",
      "Processing 47495_coordinates.csv...\n",
      "47495_coordinates.csv.pt exists.\n",
      "Processing 43439_coordinates.csv...\n",
      "43439_coordinates.csv.pt exists.\n",
      "Processing 42774_coordinates.csv...\n",
      "42774_coordinates.csv.pt exists.\n",
      "Processing 44635_coordinates.csv...\n",
      "44635_coordinates.csv.pt exists.\n",
      "Processing 48336_coordinates.csv...\n",
      "48336_coordinates.csv.pt exists.\n",
      "Processing 41984_coordinates.csv...\n",
      "41984_coordinates.csv.pt exists.\n",
      "Processing 47684_coordinates.csv...\n",
      "47684_coordinates.csv.pt exists.\n",
      "Processing 25735_coordinates.csv...\n",
      "25735_coordinates.csv.pt exists.\n",
      "Processing 44428_coordinates.csv...\n",
      "44428_coordinates.csv.pt exists.\n",
      "Processing 41995_coordinates.csv...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing 41995_coordinates.csv: 5it [00:27,  5.49s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed result shape for 41995_coordinates.csv: torch.Size([8697, 4096])\n",
      "Processing 47499_coordinates.csv...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing 47499_coordinates.csv: 5it [00:27,  5.48s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed result shape for 47499_coordinates.csv: torch.Size([8610, 4096])\n",
      "Processing 24097_coordinates.csv...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing 24097_coordinates.csv: 5it [00:26,  5.34s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed result shape for 24097_coordinates.csv: torch.Size([8491, 4096])\n",
      "Processing 40048_coordinates.csv...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing 40048_coordinates.csv: 5it [00:26,  5.28s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed result shape for 40048_coordinates.csv: torch.Size([8272, 4096])\n",
      "Processing 26231_coordinates.csv...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing 26231_coordinates.csv: 5it [00:27,  5.40s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed result shape for 26231_coordinates.csv: torch.Size([8687, 4096])\n",
      "Processing 46085_coordinates.csv...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing 46085_coordinates.csv: 5it [00:27,  5.51s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed result shape for 46085_coordinates.csv: torch.Size([8688, 4096])\n",
      "Processing 45123_coordinates.csv...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing 45123_coordinates.csv: 5it [00:26,  5.39s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed result shape for 45123_coordinates.csv: torch.Size([8642, 4096])\n",
      "Processing 41602_coordinates.csv...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing 41602_coordinates.csv: 5it [00:27,  5.41s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed result shape for 41602_coordinates.csv: torch.Size([8351, 4096])\n",
      "Processing 41988_coordinates.csv...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing 41988_coordinates.csv: 5it [00:26,  5.39s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed result shape for 41988_coordinates.csv: torch.Size([8632, 4096])\n",
      "Processing 42788_coordinates.csv...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing 42788_coordinates.csv: 5it [00:26,  5.39s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed result shape for 42788_coordinates.csv: torch.Size([8638, 4096])\n",
      "Processing 37853_coordinates.csv...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing 37853_coordinates.csv: 5it [00:27,  5.41s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed result shape for 37853_coordinates.csv: torch.Size([8709, 4096])\n",
      "Processing 37852_coordinates.csv...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing 37852_coordinates.csv: 5it [00:27,  5.44s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed result shape for 37852_coordinates.csv: torch.Size([8459, 4096])\n",
      "Processing 42775_coordinates.csv...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing 42775_coordinates.csv: 0it [00:06, ?it/s]\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 2.13 GiB (GPU 2; 39.49 GiB total capacity; 31.50 GiB already allocated; 1.07 GiB free; 37.92 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 46\u001b[0m\n\u001b[1;32m     44\u001b[0m start_idx \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m     45\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m idx, data \u001b[38;5;129;01min\u001b[39;00m tqdm(\u001b[38;5;28menumerate\u001b[39m(data_loader), desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mProcessing \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m---> 46\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Ensure data is on the model's device\u001b[39;00m\n\u001b[1;32m     47\u001b[0m     batch_size \u001b[38;5;241m=\u001b[39m output\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     48\u001b[0m     result[start_idx:start_idx \u001b[38;5;241m+\u001b[39m batch_size] \u001b[38;5;241m=\u001b[39m output\n",
      "File \u001b[0;32m~/python_install/lib/python3.8/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/AutoTimes-main/models/Preprocess_DeepSeek.py:40\u001b[0m, in \u001b[0;36mModel.forward\u001b[0;34m(self, x_mark_enc)\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x_mark_enc):\n\u001b[0;32m---> 40\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforecast\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_mark_enc\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/AutoTimes-main/models/Preprocess_DeepSeek.py:35\u001b[0m, in \u001b[0;36mModel.forecast\u001b[0;34m(self, x_mark_enc)\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforecast\u001b[39m(\u001b[38;5;28mself\u001b[39m, x_mark_enc):        \n\u001b[1;32m     33\u001b[0m     \u001b[38;5;66;03m# x_mark_enc: [bs x T x hidden_dim_of_llama]\u001b[39;00m\n\u001b[1;32m     34\u001b[0m     x_mark_enc \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat([\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokenizer(x_mark_enc[i]) \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(x_mark_enc))], \u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m---> 35\u001b[0m     text_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mllama\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mx_mark_enc\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     36\u001b[0m     text_outputs \u001b[38;5;241m=\u001b[39m text_outputs[:, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, :]\n\u001b[1;32m     37\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m text_outputs\n",
      "File \u001b[0;32m~/python_install/lib/python3.8/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/python_install/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:945\u001b[0m, in \u001b[0;36mLlamaModel.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict, cache_position)\u001b[0m\n\u001b[1;32m    933\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(\n\u001b[1;32m    934\u001b[0m         decoder_layer\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m,\n\u001b[1;32m    935\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    942\u001b[0m         position_embeddings,\n\u001b[1;32m    943\u001b[0m     )\n\u001b[1;32m    944\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 945\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mdecoder_layer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    946\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    947\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcausal_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    948\u001b[0m \u001b[43m        \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    949\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    950\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    951\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    952\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    953\u001b[0m \u001b[43m        \u001b[49m\u001b[43mposition_embeddings\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_embeddings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    954\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    956\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    958\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache:\n",
      "File \u001b[0;32m~/python_install/lib/python3.8/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/python_install/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:692\u001b[0m, in \u001b[0;36mLlamaDecoderLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache, cache_position, position_embeddings, **kwargs)\u001b[0m\n\u001b[1;32m    690\u001b[0m residual \u001b[38;5;241m=\u001b[39m hidden_states\n\u001b[1;32m    691\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpost_attention_layernorm(hidden_states)\n\u001b[0;32m--> 692\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmlp\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    693\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m residual \u001b[38;5;241m+\u001b[39m hidden_states\n\u001b[1;32m    695\u001b[0m outputs \u001b[38;5;241m=\u001b[39m (hidden_states,)\n",
      "File \u001b[0;32m~/python_install/lib/python3.8/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/python_install/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:258\u001b[0m, in \u001b[0;36mLlamaMLP.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    256\u001b[0m     down_proj \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msum\u001b[39m(down_proj)\n\u001b[1;32m    257\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 258\u001b[0m     down_proj \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdown_proj(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mact_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgate_proj\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mup_proj\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    260\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m down_proj\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 2.13 GiB (GPU 2; 39.49 GiB total capacity; 31.50 GiB already allocated; 1.07 GiB free; 37.92 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "for file_name in os.listdir(data_dir):\n",
    "    if file_name.endswith(\".csv\"):\n",
    "        print(f\"Processing {file_name}...\")\n",
    "        file_pathh = os.path.join(save_dir_path, f'{file_name}.pt')\n",
    "        if os.path.exists(file_pathh):\n",
    "            print(f'{file_name}.pt exists.')\n",
    "            continue\n",
    "        # Dataset preprocessing\n",
    "        data_set = Dataset_Preprocess_Sat(\n",
    "            root_path=data_dir,\n",
    "            data_path=file_name,\n",
    "            size=[seq_len, label_len, pred_len]\n",
    "        )\n",
    "\n",
    "        # Data loader for batching\n",
    "        data_loader = DataLoader(\n",
    "            data_set,\n",
    "            batch_size=2048,\n",
    "            num_workers=0,\n",
    "            shuffle=False,\n",
    "        )\n",
    "\n",
    "        # Process the data with the model\n",
    "        #output_list = []\n",
    "        #with torch.no_grad():\n",
    "        #    for idx, data in tqdm(enumerate(data_loader), desc=f\"Processing {file_name}\"):\n",
    "        #        #print(data[0:seq_len:pred_len],len(data))\n",
    "        #        output = model(data)\n",
    "        #        print(output.shape)\n",
    "        #        output_list.append(output.detach().cpu())\n",
    "        #    print(len(output_list))\n",
    "\n",
    "        #Concatenate and save the result\n",
    "        #result = torch.cat(output_list, dim=0)\n",
    "        #print(f\"Processed result shape for {file_name}: {result.shape}\")\n",
    "        #torch.save(result, os.path.join(save_dir_path, f'{file_name}.pt'))\n",
    "\n",
    "        total_samples = len(data_set)\n",
    "        # Assume you know the output shape per sample, e.g., (batch_size, output_dim)\n",
    "        # Replace 'output_shape' with actual dims (excluding batch dim)\n",
    "        result = torch.empty((total_samples, 4096), device='cuda' if torch.cuda.is_available() else 'cpu')  # Preallocate on device\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            start_idx = 0\n",
    "            for idx, data in tqdm(enumerate(data_loader), desc=f\"Processing {file_name}\"):\n",
    "                output = model(data)  # Ensure data is on the model's device\n",
    "                batch_size = output.shape[0]\n",
    "                result[start_idx:start_idx + batch_size] = output\n",
    "                start_idx += batch_size\n",
    "        \n",
    "        result = result.cpu()  # Move to CPU only once at the end\n",
    "        print(f\"Processed result shape for {file_name}: {result.shape}\")\n",
    "\n",
    "        \n",
    "\n",
    "print(f\"All datasets processed and saved in {save_dir_path}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "674fe993",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 3]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a=[1,2,3,4]\n",
    "a[1:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48099105",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
